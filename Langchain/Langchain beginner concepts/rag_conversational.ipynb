{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 11, 'total_tokens': 37, 'completion_time': 0.019846164, 'prompt_time': 0.002626493, 'queue_time': None, 'total_time': 0.022472657}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_af05557ca2', 'finish_reason': 'stop', 'logprobs': None}, id='run-1a4ca57c-ade2-4fd4-a084-a37d4712adf8-0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = os.environ[\"PINECONE_API_KEY\"]\n",
    "index_name = \"books-pinecone-with-metadata\"\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"For Further Exploration\\nTo learn more about LangChain and its wide range of applications, be sure to check out the comprehensive documentation and tutorials available on the official website:\\n\\nLangChain Documentation: https://python.langchain.com/\\nDon't Forget to Like and Subscribe!\\nIf you're looking for in-depth tutorials and insights into LangChain, CrewAI, and other AI technologies, be sure to check out the fantastic YouTube channel by Brandon Hancock:\\n\\nYouTube Channel: https://www.youtube.com/@bhancock_ai\\nDon't forget to like and subscribe to his channel!!\", metadata={'source': 'langchain_demo.txt'}),\n",
       " Document(page_content='LangChain: A Framework for LLM-Powered Applications\\nLangChain is a powerful and flexible framework designed to simplify the development of applications that harness the capabilities of large language models (LLMs). It provides a wide range of tools, abstractions, and integrations that help developers build, customize, and optimize applications that leverage LLMs for tasks like text generation, question answering, summarization, chatbots, and more.', metadata={'source': 'langchain_demo.txt'}),\n",
       " Document(page_content='Key Features and Benefits\\nModular Components: LangChain offers a variety of modular components (chains, agents, tools, prompts, memory, etc.) that can be easily combined and customized to build complex LLM-powered workflows.\\nData Integration: It seamlessly integrates with various data sources, enabling applications to access and process external information, enhancing the context and relevance of LLM responses.\\nAgent Frameworks: LangChain provides agent frameworks that allow LLMs to interact with their environment, make decisions, and take actions based on user input or specific goals.\\nMemory Management: It includes memory components that enable applications to maintain context and track conversations, leading to more coherent and personalized interactions.\\nPrompt Engineering: LangChain facilitates prompt engineering, the process of crafting effective prompts to elicit desired responses from LLMs, by offering templates and tools for experimentation.\\nChain Optimization: It provides mechanisms to evaluate and optimize chain performance, ensuring that applications deliver the best possible results.\\nUse Cases\\nLangChain empowers developers to create a wide array of applications, including:', metadata={'source': 'langchain_demo.txt'}),\n",
       " Document(page_content='CHAPTER 15. Chowder.', metadata={'source': 'moby_dick.txt'})]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\":2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"For Further Exploration\\nTo learn more about LangChain and its wide range of applications, be sure to check out the comprehensive documentation and tutorials available on the official website:\\n\\nLangChain Documentation: https://python.langchain.com/\\nDon't Forget to Like and Subscribe!\\nIf you're looking for in-depth tutorials and insights into LangChain, CrewAI, and other AI technologies, be sure to check out the fantastic YouTube channel by Brandon Hancock:\\n\\nYouTube Channel: https://www.youtube.com/@bhancock_ai\\nDon't forget to like and subscribe to his channel!!\", metadata={'source': 'langchain_demo.txt'}),\n",
       " Document(page_content='LangChain: A Framework for LLM-Powered Applications\\nLangChain is a powerful and flexible framework designed to simplify the development of applications that harness the capabilities of large language models (LLMs). It provides a wide range of tools, abstractions, and integrations that help developers build, customize, and optimize applications that leverage LLMs for tasks like text generation, question answering, summarization, chatbots, and more.', metadata={'source': 'langchain_demo.txt'})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextualize question prompt\n",
    "# This system prompt helps the AI understand that it should reformulate the question\n",
    "# based on the chat history to make it a standalone question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    "    \"if asked any normal question answer it.\"\n",
    "    # \"search about langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever( \n",
    "# LLM: to generate search term given chat history.\n",
    "# retriever: a simple retriever\n",
    "# contextualize_q_prompt: a simple prompt to guide llm\n",
    "    llm,retriever,contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the history aware retriever\n",
    "\n",
    "context_test = history_aware_retriever.invoke({\n",
    "    \"chat_history\": [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Paris\"\n",
    "    ],\n",
    "    \"input\": \"where is France?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='We need only confess that we do not know the purpose of the European\\nconvulsions and that we know only the facts—that is, the murders, first\\nin France, then in Italy, in Africa, in Prussia, in Austria, in Spain,\\nand in Russia—and that the movements from the west to the east and from\\nthe east to the west form the essence and purpose of these events, and\\nnot only shall we have no need to see exceptional ability and genius in\\nNapoleon and Alexander, but we shall be unable to consider them to\\nbe anything but like other men, and we shall not be obliged to have\\nrecourse to chance for an explanation of those small events which made\\nthese people what they were, but it will be clear that all those small\\nevents were inevitable.', metadata={'source': 'war_and_peace.txt'})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question prompt\n",
    "# This system prompt helps the AI understand that it should provide concise answers\n",
    "# based on the retrieved context and indicates what to do if the answer is unknown\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. Use \"\n",
    "    \"the following pieces of retrieved context to answer the \"\n",
    "    \"question. If you don't know the answer, just say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the answer \"\n",
    "    \"concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\" # This is where the retrieved context will be placed after the histroy aware retriever chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_ans_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for passing a list of Documents to a model.\n",
    "\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm,ques_ans_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000258C6BAD1C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000258C6BAE810>, model_name='llama3-8b-8192', groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a retrieval chain that combines the history-aware retriever and the QA chain\n",
    "# Create retrieval chain that retrieves documents and then passes them on.\n",
    "\n",
    "rag_chain = create_retrieval_chain(\n",
    "    history_aware_retriever, # retrieve documents\n",
    "    qa_chain # get the retrieved documents and pass it to LLM to answer the question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['PineconeVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x00000258A04A4C50>, search_kwargs={'k': 2}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='search about langchain')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "           | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000258C6BAD1C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000258C6BAE810>, model_name='llama3-8b-8192', groq_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['PineconeVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x00000258A04A4C50>, search_kwargs={'k': 2})), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000258C6BAD1C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000258C6BAE810>, model_name='llama3-8b-8192', groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def continue_chat():\n",
    "    print(\"Start Chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        \n",
    "        # process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke(\n",
    "            {\n",
    "                \"input\": query,\n",
    "                \"chat_history\": chat_history\n",
    "            }\n",
    "        )\n",
    "        print(f\"User: {query}\")\n",
    "        print(f\"AI: {result[\"answer\"]}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(AIMessage(content=result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Chatting with the AI! Type 'exit' to end the conversation.\n",
      "User: what is langchainb\n",
      "AI: LangChain is a powerful and flexible framework designed to simplify the development of applications that harness the capabilities of large language models (LLMs).\n",
      "\n",
      "\n",
      "User: who is romeo\n",
      "AI: In the context of the provided passage, Romeo is a character in William Shakespeare's play \"Romeo and Juliet\", who is involved in a fight with Tybalt, resulting in the death of Mercutio.\n",
      "\n",
      "\n",
      "User: who juliet\n",
      "AI: Juliet is the female protagonist in William Shakespeare's play \"Romeo and Juliet\", who is a member of the Capulet family and falls in love with Romeo, a Montague, despite their families' hatred for each other.\n",
      "\n",
      "\n",
      "User: how are they relatated\n",
      "AI: Romeo and Juliet are romantically involved and are from feuding families, the Montagues and the Capulets, respectively.\n",
      "\n",
      "\n",
      "User: where is odyssey\n",
      "AI: The Odyssey is set in ancient Greece, specifically in the city of Troy and the surrounding lands, as well as in various mythological locations.\n",
      "\n",
      "\n",
      "User: what was my first question answer it \n",
      "AI: Your first question was \"what is langchainb\".\n",
      "\n",
      "\n",
      "User: for what is it used now\n",
      "AI: I don't know.\n",
      "\n",
      "\n",
      "User: more info about langchain\n",
      "AI: LangChain is a framework designed to simplify the development of applications that harness the capabilities of large language models (LLMs).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    continue_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
