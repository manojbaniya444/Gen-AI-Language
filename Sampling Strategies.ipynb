{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "890ef3eab7e2404a9c7137083e2d3fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c017c5ba1916469f95dc9074ff0d6e66",
              "IPY_MODEL_b9ae9906c83e47c49d51c8ad21a91b07",
              "IPY_MODEL_5952ed1b92724d0aa39571e1ffecfbe4"
            ],
            "layout": "IPY_MODEL_60b7047e2f84498890466e694bafb7f0"
          }
        },
        "c017c5ba1916469f95dc9074ff0d6e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b474e95938d44b64a3a71d3579957adf",
            "placeholder": "​",
            "style": "IPY_MODEL_1b2c20d4421a4a36aac427edfef6b3e8",
            "value": "generation_config.json: 100%"
          }
        },
        "b9ae9906c83e47c49d51c8ad21a91b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23f1833e0df342349d9985aad4e70530",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed3a61e89a5b4d6bbb1201c146e01856",
            "value": 124
          }
        },
        "5952ed1b92724d0aa39571e1ffecfbe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb5cbc76f05c4f49b7023e7aab8518a1",
            "placeholder": "​",
            "style": "IPY_MODEL_5c104035cab54272a19b071ccdf016d6",
            "value": " 124/124 [00:00&lt;00:00, 5.70kB/s]"
          }
        },
        "60b7047e2f84498890466e694bafb7f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b474e95938d44b64a3a71d3579957adf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b2c20d4421a4a36aac427edfef6b3e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23f1833e0df342349d9985aad4e70530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed3a61e89a5b4d6bbb1201c146e01856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb5cbc76f05c4f49b7023e7aab8518a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c104035cab54272a19b071ccdf016d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "4iHTG3B5NTC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "6rjwMJ45VgPu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"gpt2\""
      ],
      "metadata": {
        "id": "C5t5yeg-Vk5g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "890ef3eab7e2404a9c7137083e2d3fe0",
            "c017c5ba1916469f95dc9074ff0d6e66",
            "b9ae9906c83e47c49d51c8ad21a91b07",
            "5952ed1b92724d0aa39571e1ffecfbe4",
            "60b7047e2f84498890466e694bafb7f0",
            "b474e95938d44b64a3a71d3579957adf",
            "1b2c20d4421a4a36aac427edfef6b3e8",
            "23f1833e0df342349d9985aad4e70530",
            "ed3a61e89a5b4d6bbb1201c146e01856",
            "bb5cbc76f05c4f49b7023e7aab8518a1",
            "5c104035cab54272a19b071ccdf016d6"
          ]
        },
        "id": "sPtGTxbdVmr4",
        "outputId": "0f696744-cfc1-43de-dd1f-b5003f3361b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "890ef3eab7e2404a9c7137083e2d3fe0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kubONvqOVsbU",
        "outputId": "9235b334-36ed-4f56-d1a5-2d362d198657"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1smHNzYHVwGV",
        "outputId": "716e2312-7983-4fd2-8e13-a4e79e74e974"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Leg0RdF4V22V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW4PZXbBV_oy",
        "outputId": "c11504aa-0f1e-4881-ea67-06a02134de4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The sky is\"\n",
        "input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "input_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUpuYhIEVxje",
        "outputId": "e17501f3-371c-465e-d81d-5ec58b552fa7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 464, 6766,  318]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(input_tokens).logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGYFEn13WRF_",
        "outputId": "8f3e6e8a-13c8-49f9-e974-dd4666a43fc8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_token_logits = model(input_tokens).logits[:, -1, :]\n",
        "last_token_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNfyxcMDWYaX",
        "outputId": "d2f0baba-bf69-4131-ba94-7ea3dee61d31"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_id_argmax = last_token_logits.argmax(dim=-1)\n",
        "next_token_id_argmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1snndfZWlP5",
        "outputId": "006dfea9-cd60-4aa1-986f-0144ee7516dc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([262], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(next_token_id_argmax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PuhGzXuGWcWl",
        "outputId": "76731f0a-eefc-4056-a9e8-973b2ea3eb70"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Text Generation using Greedy Sampling"
      ],
      "metadata": {
        "id": "C7J7dsd-WzDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_responses = 5"
      ],
      "metadata": {
        "id": "2WBNp7Gub_Up"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 30\n",
        "\n",
        "# manual greedy decoding\n",
        "for i in range(total_responses):\n",
        "  prompt = \"Once upon a time\"\n",
        "  input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids = input_tokens)\n",
        "\n",
        "    logits = output.logits[:, -1, :]\n",
        "    next_token_id = logits.argmax(dim=-1).unsqueeze(0)\n",
        "\n",
        "    input_tokens = torch.cat([input_tokens, next_token_id], dim=-1)\n",
        "\n",
        "  output_text = tokenizer.decode(input_tokens[0], skip_special_tokens=True)\n",
        "  print(f\"Response {i}: {output_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF6M55EbW0uX",
        "outputId": "4bdfb36d-37f6-4a52-c26f-f28cb9653e6c"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 0: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great\n",
            "Response 1: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great\n",
            "Response 2: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great\n",
            "Response 3: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great\n",
            "Response 4: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Text Generation using Random Sampling"
      ],
      "metadata": {
        "id": "BzJObmpXXjuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 30\n",
        "\n",
        "# manual greedy decoding\n",
        "for i in range(total_responses):\n",
        "  prompt = \"Once upon a time\"\n",
        "  input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids = input_tokens)\n",
        "\n",
        "    logits = output.logits[:, -1, :]\n",
        "    next_token_id = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)\n",
        "\n",
        "    input_tokens = torch.cat([input_tokens, next_token_id], dim=-1)\n",
        "\n",
        "  output_text = tokenizer.decode(input_tokens[0], skip_special_tokens=True)\n",
        "  print(f\"Response {i}: {output_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvGtwZ9iXm9R",
        "outputId": "c1721e5f-5a62-49d2-a6ca-44415847001c"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 0: Once upon a time, aliens replaced humans as productivity aids for the production of Tianinfuex, a sentient intelligent offspring of Buttermilk, the majority of which are\n",
            "Response 1: Once upon a time you would strive seek realism, New York was all but destroyed by Trump's Super PACs. One need only to look at the wildly unsuccessful attempts by Bill\n",
            "Response 2: Once upon a time she saw this? Perhaps the plain steel roof. As she thought, about those magnificent projection cats staying behind, one thought. This sentence drew her ly\n",
            "Response 3: Once upon a time there was nothing remarkable in birth about Qing. His four favorite people or cultivators were very intelligent, extremely gentle and sensible– as well as wise-\n",
            "Response 4: Once upon a time he was as helpless as a chicken at shaving his arms in a feather chase, as his tail with thumb stretched and now demanded to be shortened. By\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature Scaling"
      ],
      "metadata": {
        "id": "ibIY5YL6X6gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 30\n",
        "temperature = 0.2\n",
        "# lower = more conservative Higher = more random\n",
        "for i in range(total_responses):\n",
        "  prompt = \"Once upon a time\"\n",
        "  input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "  for _ in range(max_new_tokens):\n",
        "      with torch.no_grad():\n",
        "        output = model(input_ids = input_tokens)\n",
        "\n",
        "      logits = output.logits[:, -1, :]\n",
        "      logits = logits / temperature\n",
        "      next_token_id = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)\n",
        "\n",
        "      input_tokens = torch.cat([input_tokens, next_token_id], dim=-1)\n",
        "\n",
        "  output_text = tokenizer.decode(input_tokens[0], skip_special_tokens=True)\n",
        "  print(f\"Response {i}: {output_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlD6beAuXzE8",
        "outputId": "610907cd-7571-4b73-a50e-a4bbe78c5130"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 0: Once upon a time, the whole world was a land of the dead. The dead were the living, the dead were the dead. The dead were the living, the\n",
            "Response 1: Once upon a time, the world was a place of great beauty and beauty, and the world was a place of great danger. And the world was a place of great\n",
            "Response 2: Once upon a time, the world was filled with the sound of the wind, and the sound of the moon. The world was filled with the sound of the moon,\n",
            "Response 3: Once upon a time, the world was a place of great beauty, but now it is a place of darkness. The world is filled with darkness, and the darkness is\n",
            "Response 4: Once upon a time, the world was a place of peace, but now it is a place of war.\n",
            "\n",
            "The world is a place of war.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top K Sampling"
      ],
      "metadata": {
        "id": "AOqll3xeZ3tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 30\n",
        "top_k = 10\n",
        "\n",
        "for i in range(total_responses):\n",
        "  prompt = \"Once upon a\"\n",
        "  input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids = input_tokens)\n",
        "\n",
        "    logits = output.logits[:, -1, :]\n",
        "\n",
        "    # Top-k filtering\n",
        "    topk_logits, topk_indices = torch.topk(logits, k=top_k, dim=-1)\n",
        "    probs = torch.softmax(topk_logits, dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    # mapping back to the original vocab indices\n",
        "    next_token_id = topk_indices.gather(-1, next_token)\n",
        "    input_tokens = torch.cat([input_tokens, next_token_id], dim=-1)\n",
        "\n",
        "  output_text = tokenizer.decode(input_tokens[0], skip_special_tokens=True)\n",
        "  print(f\"Response {i}: {output_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ds4L360Z5cM",
        "outputId": "1a60b26c-273f-4acb-e210-c30bdfa06262"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 0: Once upon a time, you may choose to pay for all of the costs incurred by the Company in connection with the purchase.\n",
            "\n",
            "3.6\n",
            "\n",
            "3\n",
            "Response 1: Once upon a second glance at her body and the way the head held her, it wasn't a man or a woman. It was something more in tune with what\n",
            "Response 2: Once upon a sudden the light suddenly dims and disappears. This is the most common occurrence of the human mind.\n",
            "\n",
            "The reason behind this is unknown.\n",
            "\n",
            "Response 3: Once upon a time, the world's most powerful corporation, known as the U.S. government, is trying to control the world by creating a super-vill\n",
            "Response 4: Once upon a time the king of England said to me, 'Lord, I have a letter, or rather my letter,' and I said 'Oh, I am\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top P Sampling (Nucleus)"
      ],
      "metadata": {
        "id": "ZoV_NPFFd0Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling config\n",
        "max_new_tokens = 30\n",
        "top_p = 0.9  # Keep only tokens with cumulative prob <= top_p\n",
        "\n",
        "for i in range(total_responses):\n",
        "  prompt = \"Once upon a time\"\n",
        "  input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "  for _ in range(max_new_tokens):\n",
        "      with torch.no_grad():\n",
        "          output = model(input_ids=input_tokens)\n",
        "\n",
        "      logits = output.logits[:, -1, :]\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "      # Sort the probabilities descending\n",
        "      sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
        "      cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "      # Mask tokens where cumulative probability > top_p\n",
        "      sorted_mask = cumulative_probs > top_p\n",
        "      # Shift the mask right to always include the first token above the threshold\n",
        "      sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
        "      sorted_mask[..., 0] = False\n",
        "\n",
        "      # Set masked tokens' probabilities to 0\n",
        "      sorted_probs[sorted_mask] = 0\n",
        "      sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)  # re-normalize\n",
        "\n",
        "      # Sample from the filtered distribution\n",
        "      next_token = torch.multinomial(sorted_probs, num_samples=1)\n",
        "\n",
        "      # Map back to original token ID\n",
        "      next_token_id = sorted_indices.gather(-1, next_token)\n",
        "\n",
        "      input_tokens = torch.cat([input_tokens, next_token_id], dim=-1)\n",
        "\n",
        "  # Decode and print\n",
        "  output_text = tokenizer.decode(input_tokens[0], skip_special_tokens=True)\n",
        "  print(f\"Response {i}: {output_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2tk6Emid3SM",
        "outputId": "0f716bf1-1e88-43ce-f09e-c5e9804928ea"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 0: Once upon a time in this world, the scriptures speak of the separation of man and beast. It was after this manner that they say about God, the Word of God\n",
            "Response 1: Once upon a time the internal combustion engine ran on standby for 20 minutes, the heat was banished. This short delay allowed the idea to appear and move smoothly, much like\n",
            "Response 2: Once upon a time, the Bolivarian world had a special source of prosperity and opportunity. All the progress had begun before what could not have been possible with an early\n",
            "Response 3: Once upon a time, one found myself called upon to defend my country from the foul evil that were the wolves, just as they had taken refuge with us and acted under\n",
            "Response 4: Once upon a time, Vishnu thought of doing something for his friend. In those \"decades of meditation of Khandsath\" (1934-42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining all"
      ],
      "metadata": {
        "id": "tTTXxlTDebJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_responses(prompt, num_responses=3, max_new_tokens=30, temperature=1.0, top_k=0, top_p=1.0):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_texts = []\n",
        "\n",
        "    for _ in range(num_responses):\n",
        "        current_ids = input_ids.clone()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            with torch.no_grad():\n",
        "                output = model(input_ids=current_ids)\n",
        "                logits = output.logits[:, -1, :] / temperature\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "                # Top-k filtering\n",
        "                if top_k > 0:\n",
        "                    topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1)\n",
        "                    probs = torch.zeros_like(probs).scatter(-1, topk_indices, topk_probs)\n",
        "                    probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "                # Top-p (nucleus) filtering\n",
        "                if top_p < 1.0:\n",
        "                    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "                    sorted_mask = cumulative_probs > top_p\n",
        "                    sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
        "                    sorted_mask[..., 0] = False\n",
        "                    sorted_probs[sorted_mask] = 0\n",
        "                    probs = torch.zeros_like(probs).scatter(-1, sorted_indices, sorted_probs)\n",
        "                    probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "                current_ids = torch.cat([current_ids, next_token_id], dim=-1)\n",
        "\n",
        "        output_text = tokenizer.decode(current_ids[0], skip_special_tokens=True)\n",
        "        generated_texts.append(output_text)\n",
        "\n",
        "    return generated_texts"
      ],
      "metadata": {
        "id": "7qz9E0anecRc"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = generate_responses(\n",
        "    prompt=\"Once upon a time\",\n",
        "    num_responses=5,\n",
        "    max_new_tokens=50,\n",
        "    temperature=0.9,\n",
        "    top_k=50,\n",
        "    top_p=0\n",
        ")\n",
        "\n",
        "for i, text in enumerate(results, 1):\n",
        "    print(f\"--- Response {i} ---\\n{text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuhR2Qile4yG",
        "outputId": "d183ca01-276c-4039-95d6-60763dcf5c52"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Response 1 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n",
            "--- Response 2 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n",
            "--- Response 3 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n",
            "--- Response 4 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n",
            "--- Response 5 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = generate_responses(\n",
        "    prompt=\"Once upon a time\",\n",
        "    num_responses=5,\n",
        "    max_new_tokens=50,\n",
        "    temperature=0.1,\n",
        "    top_k=50,\n",
        "    top_p=1.0\n",
        ")\n",
        "\n",
        "for i, text in enumerate(results, 1):\n",
        "    print(f\"--- Response {i} ---\\n{text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y3Dws5je8AH",
        "outputId": "c32f611d-b580-495e-8cf1-3119559a444d"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Response 1 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger. The world was a place of great danger. The world was a place of great danger. The world was a place of great danger. The\n",
            "\n",
            "--- Response 2 ---\n",
            "Once upon a time, the world was a place of great beauty, and the world was a place of great fear. And the world was a place of great fear. And the world was a place of great fear. And the world was a place of great fear.\n",
            "\n",
            "--- Response 3 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. But now, the world is a place of great danger. And now, the world is a place of great danger. And now, the world is a place of great danger. And\n",
            "\n",
            "--- Response 4 ---\n",
            "Once upon a time, the world was a place of peace and harmony. But now, the world is a place of war and bloodshed. The world is a place of terror and bloodshed. The world is a place of war and bloodshed. The world is a place of\n",
            "\n",
            "--- Response 5 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = generate_responses(\n",
        "    prompt=\"Once upon a time\",\n",
        "    num_responses=5,\n",
        "    max_new_tokens=50,\n",
        "    temperature=1,\n",
        "    top_k=1,\n",
        "    top_p=1.0\n",
        ")\n",
        "\n",
        "for i, text in enumerate(results, 1):\n",
        "    print(f\"--- Response {i} ---\\n{text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZOvogXMfAbl",
        "outputId": "87d712b1-0c0e-44e9-d554-f0ce78b5e48a"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Response 1 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n",
            "--- Response 2 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n",
            "--- Response 3 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n",
            "--- Response 4 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n",
            "--- Response 5 ---\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n",
            "\n"
          ]
        }
      ]
    }
  ]
}